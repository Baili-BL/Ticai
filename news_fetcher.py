# è´¢ç»æ–°é—»æŠ“å–æ¨¡å—
# æ”¯æŒå¤šæ•°æ®æºï¼šæ–°æµªè´¢ç»ã€åŒèŠ±é¡ºã€ä¸œæ–¹è´¢å¯Œ

import time
import requests
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    import akshare as ak
except ImportError:
    ak = None

# è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
}

# ç¼“å­˜
_news_cache = {}
_cache_time = {}
NEWS_CACHE_TTL = 180  # 3åˆ†é’Ÿç¼“å­˜

# é‡å¤§åˆ©å¥½å…³é”®è¯
POSITIVE_KEYWORDS = [
    # æ”¿ç­–åˆ©å¥½
    "å›½åŠ¡é™¢", "å‘æ”¹å§”", "å·¥ä¿¡éƒ¨", "å¤®è¡Œ", "è¯ç›‘ä¼š", "è´¢æ”¿éƒ¨",
    "æ”¿ç­–æ”¯æŒ", "æ‰¶æŒ", "è¡¥è´´", "å‡ç¨", "é™è´¹", "åˆ©å¥½",
    "æˆ˜ç•¥", "è§„åˆ’", "æ„è§", "é€šçŸ¥", "æŒ‡å¯¼",
    # è¡Œä¸šåˆ©å¥½
    "çªç ´", "é¦–å‘", "é¦–åˆ›", "é¢†å…ˆ", "ç¬¬ä¸€", "é¾™å¤´",
    "è®¢å•", "ä¸­æ ‡", "ç­¾çº¦", "åˆä½œ", "æˆ˜ç•¥åè®®",
    "ä¸šç»©é¢„å¢", "å‡€åˆ©æ¶¦å¢é•¿", "è¥æ”¶å¢é•¿", "è¶…é¢„æœŸ",
    "æ¶¨ä»·", "æä»·", "ä¾›ä¸åº”æ±‚", "äº§èƒ½ç´§å¼ ",
    # èµ„é‡‘åˆ©å¥½
    "å¢æŒ", "å›è´­", "ä¸¾ç‰Œ", "å…¥è‚¡", "å¹¶è´­", "é‡ç»„",
    "åŒ—å‘èµ„é‡‘", "å¤–èµ„ä¹°å…¥", "æœºæ„è°ƒç ”",
]

# é‡å¤§åˆ©ç©ºå…³é”®è¯
NEGATIVE_KEYWORDS = [
    "ä¸‹è·Œ", "æš´è·Œ", "å¤§è·Œ", "è·³æ°´", "å´©ç›˜",
    "å‡æŒ", "æ¸…ä»“", "æŠ›å”®", "å¥—ç°",
    "äºæŸ", "ä¸‹æ»‘", "ä¸‹é™", "ä¸šç»©å˜è„¸",
    "å¤„ç½š", "ç½šæ¬¾", "è¿è§„", "è°ƒæŸ¥", "ç«‹æ¡ˆ",
    "é€€å¸‚", "ST", "*ST", "é£é™©è­¦ç¤º",
    "è¯‰è®¼", "ä»²è£", "çº çº·",
]

# è‚¡ç¥¨åç§°å¸¸è§åç¼€ï¼ˆç”¨äºæ¸…æ´—ï¼‰
STOCK_NAME_SUFFIXES = ["è‚¡ä»½", "ç§‘æŠ€", "ç”µå­", "é›†å›¢", "æ–°æ", "æ™ºèƒ½", "ä¿¡æ¯", "ç½‘ç»œ", "è½¯ä»¶", "åŒ»è¯", "ç”Ÿç‰©", "èƒ½æº", "ç”µæ°”", "æœºæ¢°", "ææ–™"]


def _get_cached(key):
    if key in _news_cache and time.time() - _cache_time.get(key, 0) < NEWS_CACHE_TTL:
        return _news_cache[key]
    return None


def _set_cache(key, value):
    _news_cache[key] = value
    _cache_time[key] = time.time()


# ============ å¤šæºæ–°é—»è·å– ============

def fetch_sina_news(limit: int = 30) -> List[Dict]:
    """æ–°æµªè´¢ç»"""
    news_list = []
    try:
        url = 'https://feed.mix.sina.com.cn/api/roll/get'
        params = {'pageid': '153', 'lid': '2516', 'num': str(limit), 'page': '1'}
        resp = requests.get(url, headers=HEADERS, params=params, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            for item in data.get('result', {}).get('data', [])[:limit]:
                news_list.append({
                    "title": item.get('title', ''),
                    "content": item.get('intro', '') or item.get('title', ''),
                    "time": item.get('ctime', ''),
                    "source": "æ–°æµª"
                })
    except Exception as e:
        print(f"æ–°æµªè´¢ç»è·å–å¤±è´¥: {e}")
    return news_list


def fetch_ths_news(limit: int = 30) -> List[Dict]:
    """åŒèŠ±é¡º"""
    news_list = []
    try:
        url = 'https://news.10jqka.com.cn/tapp/news/push/stock/'
        params = {'page': '1', 'tag': '', 'track': 'website', 'pagesize': str(limit)}
        resp = requests.get(url, headers={**HEADERS, 'Referer': 'https://news.10jqka.com.cn/'}, params=params, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            for item in data.get('data', {}).get('list', [])[:limit]:
                news_list.append({
                    "title": item.get('title', ''),
                    "content": item.get('digest', '') or item.get('title', ''),
                    "time": item.get('ctime', ''),
                    "source": "åŒèŠ±é¡º"
                })
    except Exception as e:
        print(f"åŒèŠ±é¡ºè·å–å¤±è´¥: {e}")
    return news_list


def fetch_eastmoney_news(limit: int = 30) -> List[Dict]:
    """ä¸œæ–¹è´¢å¯Œå¿«è®¯"""
    news_list = []
    try:
        # è‚¡ç¥¨å¿«è®¯
        url = 'https://np-anotice-stock.eastmoney.com/api/security/ann'
        params = {'sr': '-1', 'page_size': str(limit), 'page_index': '1', 'ann_type': 'A', 'client_source': 'web', 'f_node': '0'}
        resp = requests.get(url, headers={**HEADERS, 'Referer': 'https://data.eastmoney.com/'}, params=params, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            items = data.get('data', {}).get('list', []) if data.get('data') else []
            for item in items[:limit]:
                title = item.get('NOTICETITLE', '')
                # è¿‡æ»¤æ‰çº¯å…¬å‘Šç±»
                if title and not any(x in title for x in ['æ‹›è‚¡', 'å®¡è®¡', 'ç« ç¨‹', 'è®®æ¡ˆ']):
                    news_list.append({
                        "title": title,
                        "content": title,
                        "time": item.get('NOTICETIME', ''),
                        "source": "ä¸œè´¢"
                    })
    except Exception as e:
        print(f"ä¸œæ–¹è´¢å¯Œè·å–å¤±è´¥: {e}")
    return news_list


def fetch_all_news(limit_per_source: int = 30) -> List[Dict]:
    """
    å¹¶å‘è·å–å¤šæºæ–°é—»
    """
    cache_key = f"all_news_{limit_per_source}"
    cached = _get_cached(cache_key)
    if cached:
        return cached
    
    all_news = []
    
    # å¹¶å‘è·å–
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {
            executor.submit(fetch_sina_news, limit_per_source): "æ–°æµª",
            executor.submit(fetch_ths_news, limit_per_source): "åŒèŠ±é¡º",
            executor.submit(fetch_eastmoney_news, limit_per_source): "ä¸œè´¢",
        }
        
        for future in as_completed(futures, timeout=15):
            source = futures[future]
            try:
                news = future.result()
                all_news.extend(news)
                print(f"  {source}: {len(news)}æ¡")
            except Exception as e:
                print(f"  {source}è·å–å¤±è´¥: {e}")
    
    # å»é‡ï¼ˆæŒ‰æ ‡é¢˜ï¼‰
    seen_titles = set()
    unique_news = []
    for news in all_news:
        title = news.get('title', '')[:20]  # å–å‰20å­—ä½œä¸ºå»é‡key
        if title and title not in seen_titles:
            seen_titles.add(title)
            unique_news.append(news)
    
    if unique_news:
        _set_cache(cache_key, unique_news)
        print(f"ğŸ“° æ–°é—»èšåˆå®Œæˆ: å…±{len(unique_news)}æ¡ (å»é‡å)")
    
    return unique_news


def fetch_cls_news(limit: int = 50) -> List[Dict]:
    """
    è·å–è´¢ç»æ–°é—»ï¼ˆå¤šæºèšåˆï¼‰
    """
    return fetch_all_news(limit // 2)  # æ¯ä¸ªæºå–ä¸€åŠ


def analyze_news_sentiment(news_list: List[Dict]) -> Dict:
    """
    åˆ†ææ–°é—»æ•´ä½“æƒ…ç»ª
    """
    if not news_list:
        return {"sentiment": "neutral", "score": 50, "positive_count": 0, "negative_count": 0}
    
    positive_count = 0
    negative_count = 0
    
    for news in news_list:
        content = news.get("content", "") + news.get("title", "")
        
        has_positive = any(kw in content for kw in POSITIVE_KEYWORDS)
        has_negative = any(kw in content for kw in NEGATIVE_KEYWORDS)
        
        if has_positive:
            positive_count += 1
        if has_negative:
            negative_count += 1
    
    # è®¡ç®—æƒ…ç»ªåˆ†æ•° (0-100)
    total = positive_count + negative_count
    if total == 0:
        score = 50
    else:
        score = int(50 + (positive_count - negative_count) / total * 50)
    score = max(0, min(100, score))
    
    if score >= 65:
        sentiment = "positive"
    elif score <= 35:
        sentiment = "negative"
    else:
        sentiment = "neutral"
    
    return {
        "sentiment": sentiment,
        "score": score,
        "positive_count": positive_count,
        "negative_count": negative_count,
    }


def extract_theme_keywords(theme_name: str, stocks: List[Dict] = None) -> List[str]:
    """
    åŠ¨æ€æå–é¢˜æå…³é”®è¯ï¼Œæ— éœ€æ‰‹åŠ¨ç»´æŠ¤æ˜ å°„è¡¨
    
    ç­–ç•¥ï¼š
    1. é¢˜æåç§°æœ¬èº«
    2. æ™ºèƒ½æ‹†åˆ†é¢˜æåï¼ˆå¦‚"å«æ˜Ÿäº’è”ç½‘"â†’"å«æ˜Ÿäº’è”ç½‘"ã€"å«æ˜Ÿ"ã€"äº’è”ç½‘"ï¼‰
    3. æˆåˆ†è‚¡åç§°ï¼ˆå»é™¤å¸¸è§åç¼€ï¼‰
    """
    keywords = []
    
    # 1. é¢˜æåç§°æœ¬èº«ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    keywords.append(theme_name)
    
    # 2. æ™ºèƒ½æ‹†åˆ†é¢˜æå
    if len(theme_name) >= 4:
        common_roots = ["äº’è”", "æ™ºèƒ½", "æ–°èƒ½", "åŠå¯¼", "èŠ¯ç‰‡", "æœºå™¨", "å«æ˜Ÿ", "ä½ç©º", "é‡å­", "ç”Ÿç‰©", "åŒ»è¯", "å†›å·¥", "æ¶ˆè´¹", "é‡‘è"]
        
        for i in range(2, len(theme_name) - 1):
            part1 = theme_name[:i]
            part2 = theme_name[i:]
            
            if len(part1) >= 2 and len(part2) >= 2:
                if part1 in common_roots or part2 in common_roots or len(part1) >= 3 or len(part2) >= 3:
                    keywords.append(part1)
                    keywords.append(part2)
    
    # 3. ä»æˆåˆ†è‚¡åç§°æå–å…³é”®è¯
    if stocks:
        for stock in stocks[:8]:
            stock_name = stock.get("name", "")
            if not stock_name or len(stock_name) < 2:
                continue
            
            clean_name = stock_name
            for suffix in STOCK_NAME_SUFFIXES:
                if clean_name.endswith(suffix):
                    clean_name = clean_name[:-len(suffix)]
                    break
            
            if len(clean_name) >= 2:
                keywords.append(clean_name)
            
            if len(stock_name) >= 3:
                keywords.append(stock_name)
    
    keywords = list(set(keywords))
    keywords.sort(key=len, reverse=True)
    
    return keywords


def find_theme_related_news(theme_name: str, news_list: List[Dict], stocks: List[Dict] = None) -> List[Dict]:
    """
    æŸ¥æ‰¾ä¸ç‰¹å®šé¢˜æç›¸å…³çš„æ–°é—»
    ä½¿ç”¨åŠ¨æ€æå–çš„å…³é”®è¯è¿›è¡ŒåŒ¹é…
    """
    related_news = []
    
    # åŠ¨æ€æå–å…³é”®è¯
    keywords = extract_theme_keywords(theme_name, stocks)
    
    # æœç´¢æ–°é—»
    for news in news_list:
        content = news.get("content", "") + news.get("title", "")
        matched_keyword = None
        
        for kw in keywords:
            # è¦æ±‚å…³é”®è¯é•¿åº¦è‡³å°‘3ä¸ªå­—ç¬¦ï¼Œé¿å…è¯¯åŒ¹é…
            if len(kw) >= 3 and kw in content:
                matched_keyword = kw
                break
            # å¯¹äº2ä¸ªå­—çš„å…³é”®è¯ï¼Œè¦æ±‚æ˜¯é¢˜æåæœ¬èº«
            elif len(kw) == 2 and kw == theme_name and kw in content:
                matched_keyword = kw
                break
        
        if matched_keyword:
            # åˆ¤æ–­æ˜¯åˆ©å¥½è¿˜æ˜¯åˆ©ç©º
            is_positive = any(pk in content for pk in POSITIVE_KEYWORDS)
            is_negative = any(nk in content for nk in NEGATIVE_KEYWORDS)
            
            related_news.append({
                "content": content[:100],
                "time": news.get("time", 0),
                "is_positive": is_positive,
                "is_negative": is_negative,
                "matched_keyword": matched_keyword,
            })
    
    return related_news[:5]  # æœ€å¤šè¿”å›5æ¡


def evaluate_theme_news_factor(theme_name: str, news_list: List[Dict] = None, stocks: List[Dict] = None) -> Dict:
    """
    è¯„ä¼°é¢˜æçš„æ¶ˆæ¯é¢å› å­
    è¿”å›æ¶ˆæ¯é¢è¯„åˆ†å’Œç›¸å…³æ–°é—»
    
    å‚æ•°:
        theme_name: é¢˜æåç§°
        news_list: æ–°é—»åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™è‡ªåŠ¨è·å–ï¼‰
        stocks: é¢˜æå†…çš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆç”¨äºåŒ¹é…æ–°é—»ï¼‰
    """
    if news_list is None:
        news_list = fetch_cls_news(50)
    
    related_news = find_theme_related_news(theme_name, news_list, stocks)
    
    if not related_news:
        return {
            "score": 50,
            "level": "ä¸­æ€§",
            "level_color": "#95a5a6",
            "news_count": 0,
            "positive_news": 0,
            "negative_news": 0,
            "latest_news": [],
            "summary": "æš‚æ— ç›¸å…³æ¶ˆæ¯",
        }
    
    positive_news = sum(1 for n in related_news if n.get("is_positive"))
    negative_news = sum(1 for n in related_news if n.get("is_negative"))
    
    # è®¡ç®—æ¶ˆæ¯é¢åˆ†æ•°
    score = 50
    if positive_news > negative_news:
        score = 50 + min((positive_news - negative_news) * 15, 40)
    elif negative_news > positive_news:
        score = 50 - min((negative_news - positive_news) * 15, 40)
    
    # åˆ¤æ–­æ¶ˆæ¯é¢çº§åˆ«
    if score >= 70:
        level = "åˆ©å¥½"
        level_color = "#2ecc71"
    elif score <= 30:
        level = "åˆ©ç©º"
        level_color = "#e74c3c"
    else:
        level = "ä¸­æ€§"
        level_color = "#95a5a6"
    
    # ç”Ÿæˆæ‘˜è¦
    if positive_news > 0 and negative_news == 0:
        summary = f"è¿‘æœŸæœ‰{positive_news}æ¡åˆ©å¥½æ¶ˆæ¯"
    elif negative_news > 0 and positive_news == 0:
        summary = f"è¿‘æœŸæœ‰{negative_news}æ¡åˆ©ç©ºæ¶ˆæ¯"
    elif positive_news > 0 and negative_news > 0:
        summary = f"åˆ©å¥½{positive_news}æ¡ï¼Œåˆ©ç©º{negative_news}æ¡"
    else:
        summary = f"æœ‰{len(related_news)}æ¡ç›¸å…³æ¶ˆæ¯"
    
    return {
        "score": score,
        "level": level,
        "level_color": level_color,
        "news_count": len(related_news),
        "positive_news": positive_news,
        "negative_news": negative_news,
        "latest_news": [n["content"] for n in related_news[:3]],
        "matched_keywords": list(set(n["matched_keyword"] for n in related_news)),
        "summary": summary,
    }


def get_market_news_summary() -> Dict:
    """
    è·å–å¸‚åœºæ•´ä½“æ¶ˆæ¯é¢æ‘˜è¦
    """Â·
    news_list = fetch_cls_news(50)
    sentiment = analyze_news_sentiment(news_list)
    
    return {
        "total_news": len(news_list),
        "sentiment": sentiment["sentiment"],
        "sentiment_score": sentiment["score"],
        "positive_count": sentiment["positive_count"],
        "negative_count": sentiment["negative_count"],
        "important_positive": sentiment.get("important_positive", []),
        "important_negative": sentiment.get("important_negative", []),
    }
